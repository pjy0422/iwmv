{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.json_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting ',' delimiter: line 90837 column 9 (char 10857702)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m original_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/guest-pjy/data/0831/hotpot_cf_cleaned.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m new_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/guest-pjy/data/0831/hotpot_cf_cleaned.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m      5\u001b[0m     item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m idx\n",
      "File \u001b[0;32m~/iwmv/utils/json_utils.py:7\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/venv_311/lib/python3.11/json/__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/venv_311/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.conda/envs/venv_311/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.conda/envs/venv_311/lib/python3.11/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting ',' delimiter: line 90837 column 9 (char 10857702)"
     ]
    }
   ],
   "source": [
    "original_data_path = \"/home/guest-pjy/data/0831/hotpot_cf_cleaned.json\"\n",
    "new_data_path = \"/home/guest-pjy/data/0831/hotpot_cf_cleaned.json\"\n",
    "data = load_json(original_data_path)\n",
    "for idx, item in enumerate(data):\n",
    "    item[\"index\"] = idx\n",
    "for item in data:\n",
    "    if len(item[\"paraphrase\"]) != 10:\n",
    "        print(item[\"index\"])\n",
    "    if len(item[\"counterfactual\"]) != 9:\n",
    "        print(item[\"index\"])\n",
    "    for cf in item[\"counterfactual\"]:\n",
    "        if len(cf[\"contexts\"]) != 3:\n",
    "            print(item[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    for cf in item[\"counterfactual\"]:\n",
    "        answer = item[\"answers\"]\n",
    "        for ans in answer:\n",
    "            if cf[\"answers\"][0].lower() == ans.lower():\n",
    "                print(item[\"index\"])\n",
    "                print(item[\"question\"])\n",
    "                print(cf[\"answers\"][0])\n",
    "                print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clarify\n",
    "although\n",
    "some\n",
    "often\n",
    "never\n",
    "however\n",
    "is not\n",
    "are not\n",
    "do not\n",
    "does not\n",
    "was not\n",
    "were not\n",
    "doesn't\n",
    "although\n",
    "though\n",
    "directly\n",
    "frequently\n",
    "frequent\n",
    "while\n",
    "another\n",
    "other\n",
    "incorrectly\n",
    "incorrect\n",
    "yet\n",
    "despite\n",
    "common\n",
    "commonly\n",
    "essential for clarity\n",
    "clearly\n",
    "but\n",
    "instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: original answer -> new answer\n",
    "step 2: 단어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    new_cf = []\n",
    "    for cf in item[\"counterfactual\"]:\n",
    "        cf_answer = cf[\"answers\"][0]\n",
    "        for ans in item[\"answers\"]:\n",
    "            ans_lower = re.escape(\n",
    "                ans.lower()\n",
    "            )  # Escape special characters in the answer and make it lowercase\n",
    "            pattern = re.compile(\n",
    "                r\"\\b\" + ans_lower + r\"[\\.,;:!\\?]*\\b\", re.IGNORECASE\n",
    "            )  # Compile regex pattern to ignore punctuation\n",
    "            updated_contexts = []\n",
    "            for text in cf[\"contexts\"]:\n",
    "                updated_text = pattern.sub(\n",
    "                    cf_answer, text\n",
    "                )  # Substitute matches in the original text\n",
    "                updated_contexts.append(\n",
    "                    updated_text\n",
    "                )  # Collect the updated text\n",
    "            cf[\"contexts\"] = (\n",
    "                updated_contexts  # Replace the contexts with the updated ones\n",
    "            )\n",
    "        new_cf.append(cf)\n",
    "    data[idx][\n",
    "        \"counterfactual\"\n",
    "    ] = new_cf  # Ensure the updated counterfactuals are assigned back to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:14<00:00, 207.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of words to remove\n",
    "words_to_remove = [\n",
    "    \"clarify\",\n",
    "    \"although\",\n",
    "    \"some\",\n",
    "    \"often\",\n",
    "    \"never\",\n",
    "    \"however\",\n",
    "    \"although\",\n",
    "    \"though\",\n",
    "    \"directly\",\n",
    "    \"frequently\",\n",
    "    \"frequent\",\n",
    "    \"while\",\n",
    "    \"another\",\n",
    "    \"other\",\n",
    "    \"incorrectly\",\n",
    "    \"incorrect\",\n",
    "    \"yet\",\n",
    "    \"despite\",\n",
    "    \"common\",\n",
    "    \"commonly\",\n",
    "    \"essential for clarity\",\n",
    "    \"clearly\",\n",
    "    \"but\",\n",
    "    \"instead\",\n",
    "    \"nevertheless\",\n",
    "    \"humor\",\n",
    "    \"humorous\",\n",
    "    \"joke\",\n",
    "    \"joking\",\n",
    "    \"misunderstandings\",\n",
    "    \"misconceptions\",\n",
    "    \"confuse\",\n",
    "    \"confusing\",\n",
    "    \"confusion\",\n",
    "    \"confusions\",\n",
    "    \"confused\",\n",
    "]\n",
    "\n",
    "# Dictionary of negative phrases and their positive counterparts\n",
    "negative_to_positive = {\n",
    "    \"is not\": \"is\",\n",
    "    \"are not\": \"are\",\n",
    "    \"was not\": \"was\",\n",
    "    \"were not\": \"were\",\n",
    "    \"has not\": \"has\",\n",
    "    \"have not\": \"have\",\n",
    "    \"had not\": \"had\",\n",
    "    \"does not\": \"does\",\n",
    "    \"do not\": \"do\",\n",
    "    \"did not\": \"did\",\n",
    "    \"isn't\": \"is\",\n",
    "    \"aren't\": \"are\",\n",
    "    \"wasn't\": \"was\",\n",
    "    \"weren't\": \"were\",\n",
    "    \"hasn't\": \"has\",\n",
    "    \"haven't\": \"have\",\n",
    "    \"hadn't\": \"had\",\n",
    "    \"doesn't\": \"does\",\n",
    "    \"don't\": \"do\",\n",
    "    \"didn't\": \"did\",\n",
    "}\n",
    "\n",
    "# Compile regex for removing words (case-insensitive)\n",
    "remove_pattern = re.compile(\n",
    "    r\"\\b(?:\" + \"|\".join(map(re.escape, words_to_remove)) + r\")\\b[\\.,;:!\\?]*\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Compile regex for removing \"mis\" from words starting with \"mis\", excluding \"miss\"\n",
    "mis_pattern = re.compile(r\"\\bmis(?!s)\\w*\\b\", re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Function to replace negative phrases\n",
    "def replace_negatives(text, replacements):\n",
    "    for neg, pos in replacements.items():\n",
    "        # Adjust the regex to ignore punctuation after the negative phrase\n",
    "        text = re.sub(\n",
    "            r\"\\b\" + re.escape(neg) + r\"[\\.,;:!\\?]*\\b\",\n",
    "            pos,\n",
    "            text,\n",
    "            flags=re.IGNORECASE,\n",
    "        )\n",
    "    return text\n",
    "\n",
    "\n",
    "# Function to remove \"mis\" from words starting with \"mis\", excluding \"miss\"\n",
    "def remove_mis(text):\n",
    "    return mis_pattern.sub(\n",
    "        lambda match: match.group().replace(\"mis\", \"\", 1), text\n",
    "    )\n",
    "\n",
    "\n",
    "for idx, item in enumerate(tqdm(data)):\n",
    "    new_cf = []\n",
    "    for cf in item[\"counterfactual\"]:\n",
    "        cf_answer = cf[\"answers\"][0]\n",
    "        for ans in item[\"answers\"]:\n",
    "            pattern = re.compile(re.escape(ans), re.IGNORECASE)\n",
    "            for i, text in enumerate(cf[\"contexts\"]):\n",
    "                # Remove unwanted words and replace negative phrases\n",
    "                cleaned_text = remove_pattern.sub(\"\", text)\n",
    "                cleaned_text = replace_negatives(\n",
    "                    cleaned_text, negative_to_positive\n",
    "                )\n",
    "\n",
    "                # Remove \"mis\" from words starting with \"mis\", excluding \"miss\"\n",
    "                cleaned_text = remove_mis(cleaned_text)\n",
    "\n",
    "                # Replace the answer in the cleaned text\n",
    "                updated_text = pattern.sub(cf_answer, cleaned_text)\n",
    "\n",
    "                # Update the context with the replaced and cleaned text\n",
    "                cf[\"contexts\"][\n",
    "                    i\n",
    "                ] = (\n",
    "                    updated_text.strip()\n",
    "                )  # strip() removes leading/trailing spaces\n",
    "        new_cf.append(cf)\n",
    "    data[idx][\"counterfactual\"] = new_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(new_data_path, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.json_utils import *\n",
    "\n",
    "original_data_path = \"/home/guest-pjy/data/0831/hotpot_cf_cleaned.json\"\n",
    "data = load_json(original_data_path)\n",
    "for idx, item in enumerate(data):\n",
    "    item[\"index\"] = idx\n",
    "for item in data:\n",
    "    if len(item[\"paraphrase\"]) != 10:\n",
    "        print(item[\"index\"])\n",
    "    if len(item[\"counterfactual\"]) != 9:\n",
    "        print(item[\"index\"])\n",
    "    for cf in item[\"counterfactual\"]:\n",
    "        if len(cf[\"contexts\"]) != 3:\n",
    "            print(item[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
